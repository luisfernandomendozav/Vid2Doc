# VideoDocGen

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

VideoDocGen is an open-source tool that converts MP4 video files into detailed, step-by-step documentation. It extracts audio, transcribes it into text using OpenAI's Whisper API, captures screenshots, generates descriptions for each screenshot using the BLIP model from Hugging Face, and combines all this information to create comprehensive documentation. This tool is ideal for creating tutorials, walkthroughs, or any form of video content documentation.

---

## Table of Contents

- [Features](#features)
- [Demo](#demo)
- [Installation](#installation)
  - [Prerequisites](#prerequisites)
  - [Install FFmpeg](#install-ffmpeg)
  - [Clone the Repository](#clone-the-repository)
  - [Set Up Virtual Environment](#set-up-virtual-environment)
  - [Install Python Dependencies](#install-python-dependencies)
- [Configuration](#configuration)
  - [Set Up OpenAI API Key](#set-up-openai-api-key)
- [Usage](#usage)
- [Examples](#examples)
- [Dependencies](#dependencies)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)
- [Contact](#contact)

---

## Features

- **Audio Extraction**: Extracts audio from video files using FFmpeg.
- **Transcription**: Utilizes OpenAI Whisper API to transcribe audio into text.
- **Screenshot Capture**: Captures screenshots at regular intervals from the video.
- **Image Captioning**: Generates descriptions for each screenshot using the BLIP model from Hugging Face.
- **Detailed Documentation**: Combines transcriptions and image descriptions to create comprehensive, step-by-step documentation.

---

## Demo

*Coming soon!* We will provide demo videos and sample documentation generated by VideoDocGen.

---

## Installation

### Prerequisites

- **Python 3.7 or higher**: Ensure you have a compatible Python version installed.
- **OpenAI API Key**: Required for accessing OpenAI's Whisper and GPT models.
- **FFmpeg**: Must be installed and added to your system's PATH.
- **Git** (optional but recommended): For cloning the repository.
- **Internet Connection**: Required for API access and downloading models.

### Install FFmpeg

#### macOS (Homebrew)

```bash
brew install ffmpeg
```

#### Ubuntu/Debian

```bash
sudo apt-get update
sudo apt-get install ffmpeg
```

#### Windows

1. Download the latest static build from [FFmpeg official website](https://ffmpeg.org/download.html#build-windows).
2. Extract the downloaded ZIP file.
3. Add the `bin` folder to your system's PATH environment variable.

### Clone the Repository

```bash
git clone https://github.com/yourusername/VideoDocGen.git
cd VideoDocGen
```

*Replace `yourusername` with your GitHub username.*

### Set Up Virtual Environment

It's recommended to use a virtual environment to manage your Python packages.

#### Using `venv`

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Install Python Dependencies

#### Install Required Packages

```bash
pip install -r requirements.txt
```

---

## Configuration

### Set Up OpenAI API Key

VideoDocGen requires an OpenAI API key to access Whisper and GPT models.

#### Obtain an API Key

1. Sign up or log in to your OpenAI account at [OpenAI Dashboard](https://platform.openai.com/).
2. Navigate to the API section and generate a new API key.

#### Set the API Key as an Environment Variable

##### On macOS/Linux

```bash
export OPENAI_API_KEY='your-openai-api-key'
```

##### On Windows (Command Prompt)

```cmd
set OPENAI_API_KEY=your-openai-api-key
```

##### On Windows (PowerShell)

```powershell
$env:OPENAI_API_KEY="your-openai-api-key"
```

#### Using a `.env` File (Optional)

Alternatively, you can create a `.env` file in the project directory:

```env
OPENAI_API_KEY=your-openai-api-key
```

Make sure to install `python-dotenv` and that your script loads environment variables from this file.

**Security Note**: Never commit your API keys to version control.

---

## Usage

Run the script by providing the path to your MP4 video file:

```bash
python3 videoToDocumentation.py your_video_file.mp4
```

#### Example

```bash
python3 videoToDocumentation.py sample_video.mp4
```

### What Happens When You Run the Script

1. **Audio Extraction**: The script extracts audio from the provided video file.
2. **Transcription**: It sends the audio to OpenAI's Whisper API to get the transcription.
3. **Screenshot Capture**: Captures screenshots from the video at regular intervals.
4. **Image Captioning**: Uses the BLIP model to generate descriptions for each screenshot.
5. **Documentation Generation**: Combines the transcription and image descriptions to generate detailed documentation.
6. **Output**: The documentation is saved as a text file named `{video_filename}_documentation.txt`.

---

## Examples

After running the script on `sample_video.mp4`, you will find:

- **Audio Transcript**: `sample_video_audio.wav`
- **Screenshots**: Saved in the directory `sample_video_screenshots/`
- **Detailed Documentation**: `sample_video_documentation.txt`

---

## Dependencies

The project relies on the following major dependencies:

- **[OpenAI](https://pypi.org/project/openai/)**: For accessing Whisper and GPT models.
- **[LangChain](https://pypi.org/project/langchain/)**: To build the language model chain.
- **[ffmpeg-python](https://pypi.org/project/ffmpeg-python/)**: Python bindings for FFmpeg.
- **[Transformers](https://pypi.org/project/transformers/)**: For the BLIP image captioning model.
- **[Torch](https://pypi.org/project/torch/)**: Required by Transformers for model computations.
- **[Pillow](https://pypi.org/project/Pillow/)**: For image processing tasks.
- **[Python-dotenv](https://pypi.org/project/python-dotenv/)**: To load environment variables from a `.env` file.

### Full List of Dependencies (`requirements.txt`)

```text
ffmpeg-python==0.2.0
openai==0.27.10
langchain==0.0.321
python-dotenv==1.0.0
transformers==4.34.0
torch==2.0.1
Pillow==10.0.0
```

---

## Contributing

Contributions are welcome! Please follow these steps:

1. **Fork the Repository**

   Click the "Fork" button at the top right of the repository page.

2. **Clone Your Fork**

   ```bash
   git clone https://github.com/yourusername/VideoDocGen.git
   cd VideoDocGen
   ```

3. **Create a Feature Branch**

   ```bash
   git checkout -b feature/YourFeature
   ```

4. **Make Your Changes**

   Implement your feature or bug fix.

5. **Commit Your Changes**

   ```bash
   git commit -am 'Add some feature'
   ```

6. **Push to the Branch**

   ```bash
   git push origin feature/YourFeature
   ```

7. **Open a Pull Request**

   Go to the repository on GitHub and click the "New pull request" button.

### Code of Conduct

Please adhere to the [Contributor Covenant Code of Conduct](CODE_OF_CONDUCT.md).

---

## License

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- **[OpenAI](https://openai.com)** for Whisper and GPT models.
- **[LangChain](https://github.com/hwchase17/langchain)** for the language model framework.
- **[Hugging Face](https://huggingface.co/)** for the BLIP image captioning model.
- **[FFmpeg](https://ffmpeg.org/)** for audio and video processing.
- **[ffmpeg-python](https://github.com/kkroening/ffmpeg-python)** for the Python bindings.

---

## Contact

For questions or suggestions, please open an issue or contact:

- **Email**: [your_email@example.com](mailto:your_email@example.com)
- **GitHub**: [yourusername](https://github.com/yourusername)

---

## Additional Information

### Customization Options

- **Screenshot Frequency**

  Adjust the FPS (frames per second) in the `ffmpeg` filter to change how often screenshots are taken. For example, to take a screenshot every 5 seconds, modify the `fps` parameter in the script:

  ```python
  .filter('fps', fps=1/5)
  ```

- **Prompt Template**

  Modify the `prompt` variable in `videoToDocumentation.py` to customize how the final explanation is generated.

### Performance Considerations

- **API Costs**

  Using OpenAI's Whisper and GPT APIs will incur costs. Monitor your usage to manage expenses.

- **Processing Time**

  Depending on the video length and number of screenshots, processing may take some time.

- **System Requirements**

  Running large models like BLIP may require significant RAM. Ensure your system meets these requirements.

### Security Note

Avoid hardcoding sensitive information like API keys directly into your scripts. Always use environment variables or secure credential management methods.

---

## Troubleshooting

### ModuleNotFoundError Issues

If you encounter a `ModuleNotFoundError`, ensure all required libraries are installed:

```bash
pip install -r requirements.txt
```

### FFmpeg Not Found

If you get a `FileNotFoundError: [Errno 2] No such file or directory: 'ffmpeg'`, ensure FFmpeg is installed and added to your system's PATH.

### API Errors

Verify that your OpenAI API key is correct and has the necessary permissions.

---

## Future Plans

- **Enhance Image Descriptions**

  Use more advanced image captioning models or fine-tune models for better descriptions.

- **Support for Other Video Formats**

  Extend support to other video file formats like `.avi`, `.mov`, etc.

- **GUI Development**

  Create a graphical user interface to make the tool more user-friendly.

---

## Disclaimer

This project is provided "as is" without warranty of any kind. Use it at your own risk.

---

Feel free to explore, use, and contribute to VideoDocGen! Your feedback and contributions are highly appreciated.

---

*Note: Ensure you have all the required dependencies installed and your environment properly configured before running the script.*

---

Thank you for choosing VideoDocGen!

# Vid2Doc
